{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL1_20202021_Lab_MLP_pytorch_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fnzJJDo60Y"
      },
      "source": [
        "# Lab Deep Learning / Multi-Layer Perceptron for binary-classification / in pytorch\n",
        "\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The objective of this lab is to develop a two hidden layers MLP to perform **binary classification**.\n",
        "\n",
        "We will use a MLP with 2 hidden layer with $n_{h1}=20$ and $n_{h2}=10$ hidden units and ```relu``` activation functions.\n",
        "We will perform 10000 iterations (epochs) of SGD to find the parameters.\n",
        "\n",
        "Note: for this lab, we do not separate the dataset into a train, validation and test part.\n",
        "\n",
        "### Data normalization\n",
        "\n",
        "We should normalize the data to zero mean and unit standard deviation\n",
        "\n",
        "### Model\n",
        "\n",
        "There are various ways to write NN model in pytorch. \n",
        "\n",
        "In this lab, we will write three different implementations:\n",
        "- **Model A**: manually defining the parameters (W1,b1,W2,b2,W3,b3), writing the forward equations, writting the loss equation, calling the .backward() and manually updating the weights using W1.grad. We will write the loop to perform 1000 epochs.\n",
        "- **Model B**: using the Sequential class of pytorch\n",
        "- **Model C**: a custom torch.nn.Module class for this.\n",
        "\n",
        "For Model B and C, we will use the ready made loss and optimization from the nn and optim packages.\n",
        "\n",
        "### Loss\n",
        "\n",
        "Since we are dealing with a binary classification problem, we will use a Binary Cross Entropy loss (we use ```torch.nn.BCELoss``` for Model B and C).\n",
        "\n",
        "### Parameters update/ Optimization\n",
        "\n",
        "For updating the parameters, we will use as optimizer a simple SGD algorithm (use ```torch.optim.SGD``` for Model B and C) with a learning rate of 0.1.\n",
        "\n",
        "### Backward propagation\n",
        "\n",
        "Backpropagation is automatically performed in pytorch using the ```autograd``` package. \n",
        " \n",
        "## Documentation:\n",
        "- NN: https://pytorch.org/docs/stable/nn.html\n",
        "- Autograd: https://pytorch.org/docs/stable/autograd.html\n",
        "- Optim: https://pytorch.org/docs/stable/optim.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuvU8y2Lo60Z"
      },
      "source": [
        "## Load the python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1VTuwVio60a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zof__thjo60d",
        "outputId": "38ae0658-e995-4e85-fea9-5d4bb325869b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsb-phrJo60g"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We take the usual circle dataset from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otam7ukPo60g"
      },
      "source": [
        "from sklearn import datasets\n",
        "X_np, y_np = datasets.make_circles(n_samples=1000, noise=0.2, factor=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5idAV4Co60i"
      },
      "source": [
        "We convert the ```numpy tensors``` to ```torch tensors```. \n",
        "The difference being that the latters allows to do automatic gradient differentiation (back-propagation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPxnzVSDo60j"
      },
      "source": [
        "X = torch.from_numpy(X_np).float()\n",
        "y = torch.from_numpy(y_np).float()\n",
        "y = y.view(len(y), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHgd8JYPo60l",
        "outputId": "2e21bd48-4e38-48bf-96f4-a026706e8850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(X.size())\n",
        "print(y.size())\n",
        "print(X.mean(dim=0))\n",
        "print(X.std(dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 2])\n",
            "torch.Size([1000, 1])\n",
            "tensor([-0.0037,  0.0005])\n",
            "tensor([0.5947, 0.5985])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unp-3kjjo60n"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rIwFaauo60n",
        "outputId": "f9a901c6-a7e0-4d25-fe08-4687c19cbc30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X -= X.mean(dim=0)\n",
        "X /= X.std(dim=0)\n",
        "print(X.mean(dim=0))\n",
        "print(X.std(dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-3.5763e-09, -1.8120e-08])\n",
            "tensor([1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc3VgVWOo60p"
      },
      "source": [
        "## Definition of the hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrBQMqCJo60r"
      },
      "source": [
        "n_in = X.shape[1]\n",
        "n_h1 = 20\n",
        "n_h2 = 10\n",
        "n_out = 1\n",
        "\n",
        "nb_epoch = 10000\n",
        "alpha = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIzjntgZo60t"
      },
      "source": [
        "## Model 1 (writing the network equations)\n",
        "\n",
        "Here, we will define the variables and write the equations of the network ourselves.\n",
        "However we will use ```torch tensors``` instead of ```numpy array```. \n",
        "\n",
        "***Why ?*** because torch tensors will allows us to automatically get the gradient. We will use ```loss.backward``` to launch the backpropagation from ```loss```. Then, for all tensors we created and for which we declared ```requires_grad=True```, we will get the gradient of ```loss```with respect to this variable in the field ```.grad```. \n",
        "\n",
        "***Example*** ```W1 = torch.tensors(..., requires_grad=True)``` ... ```loss.backward``` will have the gradient $\\frac{d Loss}{d W1}$in ```W1.grad```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB65rqW4o60u",
        "outputId": "5b8cc948-2ab1-4ceb-cfd9-8ff0de49fdcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# --- We first initialize the variables of the network (W1, b1, ...)\n",
        "W1 = torch.randn(n_in, n_h1) * 0.01\n",
        "W1.requires_grad = True\n",
        "b1 = torch.randn(n_h1, requires_grad=True)\n",
        "\n",
        "W2 = torch.randn(n_h1, n_h2) * 0.01\n",
        "W2.requires_grad = True\n",
        "b2 = torch.randn(n_h2, requires_grad=True)\n",
        "\n",
        "W3 = torch.randn(n_h2, n_out) * 0.01\n",
        "W3.requires_grad = True\n",
        "b3 = torch.randn(n_out, requires_grad=True)\n",
        "\n",
        "\n",
        "# --- We then write a function to perform the forward pass (using pytorch opertaors, not numpy operators)\n",
        "# --- taking X as input and returing hat_y as output\n",
        "    \n",
        "def model(X):\n",
        "      A0 = X    \n",
        "      Z1 = A0.mm(W1) + b1\n",
        "      A1 = F.relu(Z1)\n",
        "      Z2 = A1.mm(W2) + b2\n",
        "      A2 = F.relu(Z2)\n",
        "      Z3 = A2.mm(W3) + b3\n",
        "      A3 = torch.sigmoid(Z3) # F.sigmoid\n",
        "      hat_y = A3\n",
        "  return hat_y\n",
        "\n",
        "# --- We then iterate over epochs (we do not perform split into mini-batch here)\n",
        "# --- For each iteration, we\n",
        "# ---   a) perform the forward pass, \n",
        "# ---   b) compute the loss/cost, \n",
        "# ---   c) compute the backward pass to get the gradients of the cost w.r.t. the parameters W1, b1, ...\n",
        "# ---   d) perform the update of the parameters W1, b1, ...\n",
        "for num_epoch in range(0, nb_epoch):    \n",
        "\n",
        "    # --- a) Forward pass: X (n_in, N), hat_y (n_out, N)\n",
        "    hat_y = model(X)\n",
        "\n",
        "    # -- We clip hat_y in order to avoid log(0)\n",
        "    eps = 1e-10\n",
        "    hat_y = torch.clamp(hat_y, eps, 1-eps)\n",
        "    \n",
        "    # --- b) Computing the loss/cost\n",
        "    loss = -( y * torch.log(hat_y) + ( 1 - y ) * torch.log(1 - hat_y))\n",
        "    cost = torch.mean(loss)\n",
        "\n",
        "    if num_epoch % 500 == 0:\n",
        "        print('epoch {}, loss {}'.format(num_epoch, cost))\n",
        "\n",
        "    # --- c) Backward pass\n",
        "    cost.backward()\n",
        "    \n",
        "    # --- \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
        "    with torch.no_grad():\n",
        "        # --- d) perform the update of the parameters W1, b1, ...\n",
        "\n",
        "        # --- the gradients dLoss/dW1 is stored in W1.grad, dLoss/db1 is stored in b1.grad, ...\n",
        "        W1 -= alpha * W1.grad\n",
        "        b1 -= alpha * b1.grad\n",
        "        W2 -= alpha * W2.grad\n",
        "        b2 -= alpha * b2.grad\n",
        "        W3 -= alpha * W3.grad\n",
        "        b3 -= alpha * b3.grad\n",
        "\n",
        "    # --- We need to set to zero all gradients (otherwise they are cumulated)\n",
        "    W1.grad.zero_()\n",
        "    b1.grad.zero_()\n",
        "    W2.grad.zero_()\n",
        "    b2.grad.zero_()\n",
        "    W3.grad.zero_()\n",
        "    b3.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 0.7047726511955261\n",
            "epoch 500, loss 0.6931466460227966\n",
            "epoch 1000, loss 0.6931466460227966\n",
            "epoch 1500, loss 0.6931464672088623\n",
            "epoch 2000, loss 0.693146288394928\n",
            "epoch 2500, loss 0.6931461691856384\n",
            "epoch 3000, loss 0.6931459307670593\n",
            "epoch 3500, loss 0.6931458711624146\n",
            "epoch 4000, loss 0.6931455135345459\n",
            "epoch 4500, loss 0.6931440234184265\n",
            "epoch 5000, loss 0.6931401491165161\n",
            "epoch 5500, loss 0.6931331753730774\n",
            "epoch 6000, loss 0.6931193470954895\n",
            "epoch 6500, loss 0.6930903792381287\n",
            "epoch 7000, loss 0.6930228471755981\n",
            "epoch 7500, loss 0.6928315758705139\n",
            "epoch 8000, loss 0.6919697523117065\n",
            "epoch 8500, loss 0.6748167872428894\n",
            "epoch 9000, loss 0.5432940721511841\n",
            "epoch 9500, loss 0.5254406332969666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxgA4DXVo60w"
      },
      "source": [
        "## Model 2 (using nn.sequential)\n",
        "\n",
        "Here, we will use the package ```torch.nn``` which comes with a predefined set of layers. The syntax is close to the one of ```keras```(```Sequential```), but differs in the fact that layers are splitted into the matrix multiplication followed by a non-linear activations (```keras```merge both using the ```Dense```layers).\n",
        "\n",
        "The model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```. It is therefore a convenient way to write simple sequential networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elGQpQzjo60x"
      },
      "source": [
        "my_model = nn.Sequential(\n",
        "  nn.Linear(X.shape[1], n_h1),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(n_h1, n_h2),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(n_h2, n_out),\n",
        "  nn.Sigmoid()\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4twpnbEAo60z"
      },
      "source": [
        "## Model 3 (using a class definition)\n",
        "\n",
        "Here, we will write the network using the recommended pytroch way; i.e. by defining a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNsNkq9Do60z"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_in, n_h1, n_h2, n_out):\n",
        "        super(Net, self).__init__()\n",
        "        self.linear1 = nn.Linear(n_in, n_h1)  # hidden layer 1\n",
        "        self.linear2 = nn.Linear(n_h1, n_h2)  # hidden layer 2\n",
        "        self.linear3 = nn.Linear(n_h2, n_out) # output layer\n",
        "        \n",
        "    def forward(self, X):\n",
        "        A0 = X\n",
        "        A1 = F.relu(self.linear1(A0))    # activation function for hidden layer 1\n",
        "        A2 = F.relu(self.linear2(A1))    # activation function for hidden layer 2\n",
        "        A3 = F.sigmoid(self.linear3(A2)) # activation function for output layer\n",
        "        return A3\n",
        "\n",
        "my_model = Net(n_in, n_h1, n_h2, n_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts4aVeIPo601"
      },
      "source": [
        "## Criterion and Optimization for model 2 and model 3\n",
        "\n",
        "The code of Model 1 is self-contained, i.e. it already contains all necessary instruction to perform forawrd, loss, backward and parameter updates.\n",
        "\n",
        "When using ```nn.sequential``` (model 2) or a class definition of the network (model 3), we still need to define \n",
        "- what we will minimize (the loss to be minimized, i.e. Binary-Cross-Entropy). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)\n",
        "- how we will minimize the loss, i.e. what parameter update algorithms we will use (SGD, momentum). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs63V-Wgo602"
      },
      "source": [
        "criterion = nn.BCELoss(weight=None, reduction='mean')\n",
        "optimizer = optim.SGD(my_model.parameters(), lr=alpha, momentum=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL7ePZi9o604"
      },
      "source": [
        "## Training for model 2 and 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XamuBM_ho604"
      },
      "source": [
        "Having defined the network, the citerion to be minimized and the optimizer, we then perform a loop over epochs (iterations); at each step we\n",
        "- compute the forward pass by passing the data to the model: ```haty = model(x)```\n",
        "- compute the the loss (the criterion)\n",
        "- putting at zero the gradients of all the parameters of the network (this is important since, by default, pytorch accumulate the gradients over time)\n",
        "- computing the backpropagation (using as before ```.backward()```)\n",
        "- performing one step of optimization (using ```.step()```)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKfrD8V3o605",
        "outputId": "17d7ced1-e98d-4fcb-902f-26ae09152030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "source": [
        "loss_l = []\n",
        "for num_epoch in range(nb_epoch):  \n",
        "    hat_y = my_model(X) # Forward pass: Compute predicted y by passing  x to the model          \n",
        "    loss = criterion(hat_y, y) # Compute loss \n",
        "    # Zero gradients, perform a backward pass, and update the weights. \n",
        "    optimizer.zero_grad() # re-init the gradients (otherwise they are cumulated)\n",
        "    loss.backward() # perform back-propagation\n",
        "    optimizer.step() # update the weights\n",
        "    loss_l.append(loss)\n",
        "\n",
        "    if num_epoch % 500 == 0:\n",
        "        print('epoch {}, loss {}'.format(num_epoch, loss.item()))\n",
        "\n",
        "plt.plot(loss_l)        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 0.7215383648872375\n",
            "epoch 500, loss 0.2508355677127838\n",
            "epoch 1000, loss 0.23880408704280853\n",
            "epoch 1500, loss 0.23814058303833008\n",
            "epoch 2000, loss 0.23781944811344147\n",
            "epoch 2500, loss 0.2375287115573883\n",
            "epoch 3000, loss 0.23733723163604736\n",
            "epoch 3500, loss 0.2372179478406906\n",
            "epoch 4000, loss 0.23701706528663635\n",
            "epoch 4500, loss 0.2369471788406372\n",
            "epoch 5000, loss 0.23687951266765594\n",
            "epoch 5500, loss 0.23682963848114014\n",
            "epoch 6000, loss 0.23677611351013184\n",
            "epoch 6500, loss 0.23673272132873535\n",
            "epoch 7000, loss 0.23667964339256287\n",
            "epoch 7500, loss 0.2366211861371994\n",
            "epoch 8000, loss 0.23656870424747467\n",
            "epoch 8500, loss 0.2365127056837082\n",
            "epoch 9000, loss 0.2364535629749298\n",
            "epoch 9500, loss 0.2363935112953186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa7107703c8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
